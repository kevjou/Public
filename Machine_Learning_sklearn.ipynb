{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6196b7c",
   "metadata": {},
   "source": [
    "# Title: Training ML Algorithms on Islet Isolation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2ed2b",
   "metadata": {},
   "source": [
    "### Abstract:\n",
    "Islet isolations are a process by which the islet producing beta cells are isolated from the whole pancreas via enzyme digestion and high speed centrifugation.  The current network within the US for organ procurement is as follows, a hospital has a registered donor who has passed within the timeframe eligible for organ donation.  The hospital then contacts a partnered organ procurement organization which generally covers that geographical area, who then contacts potential processing centers such as ours as City of Hope to see whether or not we are interested in recieving the organs for research or transplant \n",
    "    \n",
    "In order to ensure the proper use of the organ, various demographic information about the donor is released in a controlled manner through the proper channels so that processing centers have the knowledge and choice to reject donors if they do not have an applicable use.  Over time a somewhat anectodal judgement system for gauging potential donors has developed, such as rejecting donors who have high creatine levels, or donors with unknown downtime.  Although this anectodal information is based on indirect scientific studies of the pancreas [1] [2], these experimental analyses are not currently applied or provided prior to organ acceptance.  If a predictor could be made using more easily collectible information such as general bloodwork, demographic, or lifestyle information instead, it would be easier to integrate existing medical records to these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7604301",
   "metadata": {},
   "source": [
    "### Research Question:\n",
    "Is it possible to train a machine learning algorithm to predict islet isolation yield using only donor demographic characteristics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2435503",
   "metadata": {},
   "source": [
    "### Hypothesis:\n",
    "If the data is properly scaled an encoded so that the relevant categorical information can be properly valued by the machine learning algorithm, then it should be possible to train an algorithm to predict islet yield.  The accuracy of the algorithm may be limited by the sample size and data collection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a6f01",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ea9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load excel sheet with data\n",
    "data = pd.read_excel('Hu500++ isolation data.xlsx', skiprows=3, index_col = None, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e174a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>A(+)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>257050</td>\n",
       "      <td>83790.0</td>\n",
       "      <td>39884</td>\n",
       "      <td>381250.0</td>\n",
       "      <td>59400.0</td>\n",
       "      <td>32.596771</td>\n",
       "      <td>47.599952</td>\n",
       "      <td>1086.770428</td>\n",
       "      <td>517.302205</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>501</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>56</td>\n",
       "      <td>F</td>\n",
       "      <td>O(+)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>388800</td>\n",
       "      <td>477667.0</td>\n",
       "      <td>322900</td>\n",
       "      <td>616000.0</td>\n",
       "      <td>563500.0</td>\n",
       "      <td>122.856739</td>\n",
       "      <td>67.599395</td>\n",
       "      <td>4854.339431</td>\n",
       "      <td>3281.504065</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54</td>\n",
       "      <td>F</td>\n",
       "      <td>O(-)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264710</td>\n",
       "      <td>285689.0</td>\n",
       "      <td>128783</td>\n",
       "      <td>103000.0</td>\n",
       "      <td>167500.0</td>\n",
       "      <td>107.925277</td>\n",
       "      <td>45.078039</td>\n",
       "      <td>4058.082386</td>\n",
       "      <td>1829.303977</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>F</td>\n",
       "      <td>O(+)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>242000</td>\n",
       "      <td>145633.0</td>\n",
       "      <td>172950</td>\n",
       "      <td>236000.0</td>\n",
       "      <td>285250.0</td>\n",
       "      <td>60.178926</td>\n",
       "      <td>118.757424</td>\n",
       "      <td>1408.442940</td>\n",
       "      <td>1672.630561</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UW</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>46</td>\n",
       "      <td>F</td>\n",
       "      <td>O(+)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>543833</td>\n",
       "      <td>614200.0</td>\n",
       "      <td>386000</td>\n",
       "      <td>281250.0</td>\n",
       "      <td>412000.0</td>\n",
       "      <td>112.939082</td>\n",
       "      <td>62.845979</td>\n",
       "      <td>6786.740331</td>\n",
       "      <td>4265.193370</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>NaN</td>\n",
       "      <td>T2DM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Transplant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Not Processed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Missing Information</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                    1    2    3          4    5    6     7    8    9   \\\n",
       "0    500                  NaN  NaN  NaN        NaN   24    M  A(+)  5.0  8.0   \n",
       "1    501                   No  NaN   -3  Caucasian   56    F  O(+)  5.0  6.0   \n",
       "2    502                  NaN  NaN  NaN        NaN   54    F  O(-)  5.0  2.0   \n",
       "3    503                  NaN  NaN  NaN        NaN   51    F  O(+)  5.0  5.0   \n",
       "4    504                  NaN  NaN   UW  Caucasian   46    F  O(+)  5.0  8.0   \n",
       "..   ...                  ...  ...  ...        ...  ...  ...   ...  ...  ...   \n",
       "790  NaN                 T2DM  NaN  NaN        NaN  NaN  NaN   NaN  NaN  NaN   \n",
       "791  NaN           Transplant  NaN  NaN        NaN  NaN  NaN   NaN  NaN  NaN   \n",
       "792  NaN        Not Processed  NaN  NaN        NaN  NaN  NaN   NaN  NaN  NaN   \n",
       "793  NaN  Missing Information  NaN  NaN        NaN  NaN  NaN   NaN  NaN  NaN   \n",
       "794  NaN                Atlas  NaN  NaN        NaN  NaN  NaN   NaN  NaN  NaN   \n",
       "\n",
       "     ...      28        29      30        31        32          33  \\\n",
       "0    ...  257050   83790.0   39884  381250.0   59400.0   32.596771   \n",
       "1    ...  388800  477667.0  322900  616000.0  563500.0  122.856739   \n",
       "2    ...  264710  285689.0  128783  103000.0  167500.0  107.925277   \n",
       "3    ...  242000  145633.0  172950  236000.0  285250.0   60.178926   \n",
       "4    ...  543833  614200.0  386000  281250.0  412000.0  112.939082   \n",
       "..   ...     ...       ...     ...       ...       ...         ...   \n",
       "790  ...     NaN       NaN     NaN       NaN       NaN         NaN   \n",
       "791  ...     NaN       NaN     NaN       NaN       NaN         NaN   \n",
       "792  ...     NaN       NaN     NaN       NaN       NaN         NaN   \n",
       "793  ...     NaN       NaN     NaN       NaN       NaN         NaN   \n",
       "794  ...     NaN       NaN     NaN       NaN       NaN         NaN   \n",
       "\n",
       "             34           35           36  37  \n",
       "0     47.599952  1086.770428   517.302205 NaN  \n",
       "1     67.599395  4854.339431  3281.504065 NaN  \n",
       "2     45.078039  4058.082386  1829.303977 NaN  \n",
       "3    118.757424  1408.442940  1672.630561 NaN  \n",
       "4     62.845979  6786.740331  4265.193370 NaN  \n",
       "..          ...          ...          ...  ..  \n",
       "790         NaN          NaN          NaN NaN  \n",
       "791         NaN          NaN          NaN NaN  \n",
       "792         NaN          NaN          NaN NaN  \n",
       "793         NaN          NaN          NaN NaN  \n",
       "794         NaN          NaN          NaN NaN  \n",
       "\n",
       "[795 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0517a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by creating named dictionary for index of raw data\n",
    "categorical_cols = [4, 5, 6, 8, 9, 10, 11, 13, 19, 20, 21, 32]  # Indices of categorical columns\n",
    "categorical_names = {\n",
    "    4: 'Race',\n",
    "    5: 'Age',\n",
    "    6: 'Sex',\n",
    "    8: 'Height Ft.',\n",
    "    9: 'Height In.',\n",
    "    10: 'Weight (lb)',\n",
    "    11: 'BMI',\n",
    "    13: 'HbA1c(%)',\n",
    "    19: 'Pancreas weight(g)',\n",
    "    20: 'Digested pancreas weight(g)',\n",
    "    21: 'Switch time',\n",
    "    32: 'Post Culture IEQ'\n",
    "}\n",
    "categorical_cols_to_encode = [4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adcc6895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0                    1    2    3   4     5   6     7    8    9  ...  \\\n",
      "0    500                  NaN  NaN  NaN NaN  24.0 NaN  A(+)  5.0  8.0  ...   \n",
      "1    501                   No  NaN   -3 NaN  56.0 NaN  O(+)  5.0  6.0  ...   \n",
      "2    502                  NaN  NaN  NaN NaN  54.0 NaN  O(-)  5.0  2.0  ...   \n",
      "3    503                  NaN  NaN  NaN NaN  51.0 NaN  O(+)  5.0  5.0  ...   \n",
      "4    504                  NaN  NaN   UW NaN  46.0 NaN  O(+)  5.0  8.0  ...   \n",
      "..   ...                  ...  ...  ...  ..   ...  ..   ...  ...  ...  ...   \n",
      "790  NaN                 T2DM  NaN  NaN NaN   NaN NaN   NaN  NaN  NaN  ...   \n",
      "791  NaN           Transplant  NaN  NaN NaN   NaN NaN   NaN  NaN  NaN  ...   \n",
      "792  NaN        Not Processed  NaN  NaN NaN   NaN NaN   NaN  NaN  NaN  ...   \n",
      "793  NaN  Missing Information  NaN  NaN NaN   NaN NaN   NaN  NaN  NaN  ...   \n",
      "794  NaN                Atlas  NaN  NaN NaN   NaN NaN   NaN  NaN  NaN  ...   \n",
      "\n",
      "     Sex_Quartile  Height Ft._Quartile  Height In._Quartile  \\\n",
      "0               0                    1                    3   \n",
      "1               0                    1                    2   \n",
      "2               0                    1                    1   \n",
      "3               0                    1                    2   \n",
      "4               0                    1                    3   \n",
      "..            ...                  ...                  ...   \n",
      "790             0                    0                    0   \n",
      "791             0                    0                    0   \n",
      "792             0                    0                    0   \n",
      "793             0                    0                    0   \n",
      "794             0                    0                    0   \n",
      "\n",
      "     Weight (lb)_Quartile BMI_Quartile HbA1c(%)_Quartile  \\\n",
      "0                       2            1                 2   \n",
      "1                       2            2                 1   \n",
      "2                       1            1                 3   \n",
      "3                       3            4                 1   \n",
      "4                       2            2                 1   \n",
      "..                    ...          ...               ...   \n",
      "790                     0            0                 0   \n",
      "791                     0            0                 0   \n",
      "792                     0            0                 0   \n",
      "793                     0            0                 0   \n",
      "794                     0            0                 0   \n",
      "\n",
      "    Pancreas weight(g)_Quartile Digested pancreas weight(g)_Quartile  \\\n",
      "0                             1                                    2   \n",
      "1                             3                                    2   \n",
      "2                             1                                    1   \n",
      "3                             3                                    4   \n",
      "4                             2                                    3   \n",
      "..                          ...                                  ...   \n",
      "790                           0                                    0   \n",
      "791                           0                                    0   \n",
      "792                           0                                    0   \n",
      "793                           0                                    0   \n",
      "794                           0                                    0   \n",
      "\n",
      "     Switch time_Quartile  Post Culture IEQ_Quartile  \n",
      "0                       4                          1  \n",
      "1                       4                          4  \n",
      "2                       4                          3  \n",
      "3                       2                          4  \n",
      "4                       4                          4  \n",
      "..                    ...                        ...  \n",
      "790                     0                          0  \n",
      "791                     0                          0  \n",
      "792                     0                          0  \n",
      "793                     0                          0  \n",
      "794                     0                          0  \n",
      "\n",
      "[795 rows x 50 columns]\n",
      "(795, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1556: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n"
     ]
    }
   ],
   "source": [
    "#Scale data by calculating quartile distribution and replace original values with values 1-4 representing position on quartile distribution\n",
    "#allows for comparison of variables and better processing by ML algorithm\n",
    "import numpy as np\n",
    "\n",
    "for col_index in categorical_cols:\n",
    "    data.iloc[:, col_index] = pd.to_numeric(data.iloc[:, col_index], errors='coerce')\n",
    "\n",
    "# Function to get quartile category\n",
    "def get_quartile_category(value, quartiles):\n",
    "    if pd.isnull(value):\n",
    "        return 0\n",
    "    elif value <= quartiles[0]:\n",
    "        return 1\n",
    "    elif value <= quartiles[1]:\n",
    "        return 2\n",
    "    elif value <= quartiles[2]:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# Loop through each categorical column\n",
    "for col_index in categorical_cols:\n",
    "    col_name = categorical_names[col_index]\n",
    "    quartiles = np.nanpercentile(data.iloc[:, col_index], [25, 50, 75])\n",
    "    data[col_name + '_Quartile'] = data.iloc[:, col_index].apply(lambda x: get_quartile_category(x, quartiles))\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e5fdb",
   "metadata": {},
   "source": [
    "You can see the results of the quartile calculation appended as new rows to the end of the data object.  This means that the original data is still preserved in case we want to reference the specific cases while we can use the new rows as input into the machine learning algorithm.  This can be considered label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f8d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate independent variables (features) and dependent variable (target)\n",
    "X = data.iloc[:, 38:48]  # Selecting only the quartile calculated columns\n",
    "y = data.iloc[:, 49]  # IEQ Yield\n",
    "y = y.astype(np.int_)\n",
    "\n",
    "# Clean target variable of categorical values like 'Yes' and 'No'\n",
    "#label_encoder = LabelEncoder()\n",
    "#y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# One-hot encode specific categorical columns\n",
    "X_to_encode = data.iloc[:, categorical_cols_to_encode]\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "X_encoded = pd.get_dummies(X_to_encode, columns=X_to_encode.columns)\n",
    "\n",
    "# Combine encoded columns with the rest of the categorical columns\n",
    "new_index = [4,6]\n",
    "X_remaining = X.drop(X.columns[new_index], axis=1)\n",
    "X_concatenated = pd.concat([X_encoded, X_remaining], axis=1)\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e18d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (795, 10)\n",
      "Shape of y: (795,)\n",
      "Head of X:\n",
      "   Race_Quartile  Age_Quartile  Sex_Quartile  Height Ft._Quartile  \\\n",
      "0              0             1             0                    1   \n",
      "1              0             4             0                    1   \n",
      "2              0             3             0                    1   \n",
      "3              0             3             0                    1   \n",
      "4              0             2             0                    1   \n",
      "\n",
      "   Height In._Quartile  Weight (lb)_Quartile  BMI_Quartile  HbA1c(%)_Quartile  \\\n",
      "0                    3                     2             1                  2   \n",
      "1                    2                     2             2                  1   \n",
      "2                    1                     1             1                  3   \n",
      "3                    2                     3             4                  1   \n",
      "4                    3                     2             2                  1   \n",
      "\n",
      "   Pancreas weight(g)_Quartile  Digested pancreas weight(g)_Quartile  \n",
      "0                            1                                     2  \n",
      "1                            3                                     2  \n",
      "2                            1                                     1  \n",
      "3                            3                                     4  \n",
      "4                            2                                     3  \n",
      "\n",
      "Head of y:\n",
      "0    1\n",
      "1    4\n",
      "2    3\n",
      "3    4\n",
      "4    4\n",
      "Name: Post Culture IEQ_Quartile, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of X and y\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Display the first few rows of X and y\n",
    "print(\"Head of X:\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\nHead of y:\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f079a9a",
   "metadata": {},
   "source": [
    "# Training the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68d63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'max_depth': 4, 'min_samples_split': 2}\n",
      "Accuracy of the best decision tree: 29.17%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameters grid to search through\n",
    "param_grid = {\n",
    "    'max_depth': [None, 1, 2, 3, 4],  # Vary the maximum depth of the tree\n",
    "    'min_samples_split': [2, 3, 4, 5],  # Vary the minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation to find the best hyperparameters\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "best_predictions = best_estimator.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, best_predictions)\n",
    "print(f\"Accuracy of the best decision tree: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8572fc",
   "metadata": {},
   "source": [
    "Since the dataset has a relatively low number of categorical variables and samples, the grid search will use smaller increments to instead search the more relevant combinations for accuracy and efficiency.  We can then use the calculated hyperparameters and compare the accuracy with some other trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth = 3, min_samples_split = 2, random_state=2)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "y_val_proba_clf = clf.predict_proba(X_val)  # Probabilities for each class\n",
    "\n",
    "# Calculate ROC AUC score for the multiclass case\n",
    "roc_auc_clf = roc_auc_score(y_val, y_val_proba_clf, multi_class='ovr', average='macro')\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc_clf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e27314",
   "metadata": {},
   "source": [
    "The ROC AUC score of 0.5651 does not seems very high seeing as 1 shows a perfect fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cd3b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))  # Adjust the figure size as needed\n",
    "plot_tree(clf, filled=True, feature_names=X_concatenated.columns, class_names=True)\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ed6380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth = 4, min_samples_split = 2, random_state=2)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "y_val_proba_clf1 = clf.predict_proba(X_val)  # Probabilities for each class\n",
    "\n",
    "# Calculate ROC AUC score for the multiclass case\n",
    "roc_auc_clf1 = roc_auc_score(y_val, y_val_proba_clf1, multi_class='ovr', average='macro')\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc_clf1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87523c49",
   "metadata": {},
   "source": [
    "The ROC AUC score of this tree is better at 0.61, however there would still be approximately 39% of samples being misclassified by this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))  # Adjust the figure size as needed\n",
    "plot_tree(clf, filled=True, feature_names=X_concatenated.columns, class_names=True)\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Assuming you have X_train, X_test, y_train, y_test from the previous code\n",
    "\n",
    "# Create an MLPClassifier model\n",
    "mlp = MLPClassifier(alpha=1e-05, hidden_layer_sizes=(100, 50), max_iter=2000, random_state=2)\n",
    "\n",
    "# Fit the model using the training set\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "y_val_proba = mlp.predict_proba(X_val)  # Probabilities for each class\n",
    "\n",
    "# Calculate ROC AUC score for the multiclass case\n",
    "# Replace y_true and y_score with actual y_val and y_val_proba from your dataset\n",
    "roc_auc_mlp = roc_auc_score(y_val, y_val_proba, multi_class='ovr', average='macro')\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc_mlp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a9684",
   "metadata": {},
   "source": [
    "The neural network appears to perform even worse, indicating that there may be a lack of sufficient information to predict the outcomes as the algorithm has converged but does not seem to be able to improve the loss function any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da84fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', message='You passed a edgecolor/edgecolors*')\n",
    "\n",
    "\n",
    "# Assuming you have X_train, X_test, y_train, y_test from the previous code\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Choose two features to visualize decision boundaries\n",
    "feature_combinations = list(combinations(X_train.columns, 2))  # Generate combinations of feature names\n",
    "\n",
    "# Initialize PCA for dimensionality reduction (optional, if needed)\n",
    "pca = PCA(n_components=2)  # You can adjust the number of components as needed\n",
    "\n",
    "# Plot decision boundaries for each pair of features\n",
    "for feat1, feat2 in feature_combinations:\n",
    "    X_subset_train = X_train[[feat1, feat2]]\n",
    "    X_subset_test = X_test[[feat1, feat2]]\n",
    "\n",
    "    # Apply PCA for dimensionality reduction (optional)\n",
    "    X_subset_train_pca = pca.fit_transform(X_subset_train)\n",
    "    X_subset_test_pca = pca.transform(X_subset_test)\n",
    "\n",
    "    # Fit the classifier on the subset of features\n",
    "    mlp.fit(X_subset_train_pca, y_train_encoded)\n",
    "\n",
    "    # Plot decision boundaries\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plot_decision_regions(X_subset_train_pca, y_train_encoded, clf=mlp, legend=2)\n",
    "\n",
    "    # Adding axes annotations\n",
    "    plt.xlabel(feat1)\n",
    "    plt.ylabel(feat2)\n",
    "    plt.title(f'Decision Boundaries for {feat1} vs {feat2}')\n",
    "\n",
    "    # Highlighting test data\n",
    "    plt.scatter(X_subset_test_pca[:, 0], X_subset_test_pca[:, 1], c=y_test, cmap='coolwarm', label='Test data')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec96d92",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "The decision boundaries for the neural network show that some of the categorical variables that I had included were not encoded in a way that appeared to be meaningful for the algorithm, like race and sex.  It is unlikely that these two variables would significantly improve the results of the classification however. \n",
    "Other intricacies like height being stored as two variables (ft. and in.) may have affected the decision boundary drawing, but when the graph itself is visualized, it appears that the groups were still somewhat accurately grouped together by relative height.\n",
    "Overall the training of the algorithm shows results that are somewhat better than a random predicor (ROC_AUC score of 0.5), but there is definitely optimization which is missing which could further improve the accuracy of the algorithm.\n",
    "More data relating to more islet isolations would also likely help improve the accuracy of the machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ecb5c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "I believe that my hypothesis is correct, and that it is indeed possible to predict islet yield based on donor demographic information plus some medical information that can be collected non-invasively.  However, the strength of correlation of these metrics is low, likely requiring a significantly larger dataset than the one I have provided along with a more strict and standard encoding method for the categorical variables like sex and gender so that they can be more accurately accounted for when being factored into the algorithm.\n",
    "\n",
    "With the recent approval of a biological license for allogenic islet transplantation given to the University of Chicago, stronger predictive tools will likely be necessary to increase efficiency and reduce wait times for islet transplantation in the face of increased demand.  With only about 10 centers in the US performing islet isolations, it would be highly benificial to the process if the data from each center were merged and then further analysed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750b863",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1]    Komatsu, H., Qi, M., Gonzalez, N., Salgado, M., Medrano, L., Rawson, J., Orr, C., Omori, K., Isenberg, J. S., Kandeel, F., Mullen, Y., & Al-Abdullah, I. H. (2021). A Multiparametric Assessment of Human Islets Predicts Transplant Outcomes in Diabetic Mice. Journal Name, DOI: [10.1177/09636897211052291].\n",
    "\n",
    "[2]    Wong, W. K. M., Jiang, G., Sørensen, A. E., Chew, Y. V., Lee-Maynard, C., Liuwantara, D., Williams, L., O’Connell, P. J., Dalgaard, L. T., Ma, R. C., Hawthorne, W. J., Joglekar, M. V., & Hardikar, A. A. (2019). The long noncoding RNA MALAT1 predicts human islet isolation quality. JCI Insight, DOI: [10.1172/jci.insight.129299]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
